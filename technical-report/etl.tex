\section{ETL Script}\label{etl}
\subsection{Extracting}
The total storage size of the IMDB datasets and the Oscar Awards dataset we used amounts to around 6.8GB totaling around 150 million rows across all datasets. The group decided to drop the \textit{title.akas} dataset since it is expensive to operate with many strings across many languages, and the values of the dataset are not useful for the purposes of our OLAP application. 

The group chose to use all of the remaining datasets and extracted only the columns that will be relevant for the OLAP application. The IMDB datasets provide relevant information regarding the titles and genres of films, the crew and their roles, the persons, and the ratings of a certain film. Additionally, the group chose to use an Oscar Awards dataset from Kaggle to extract the films or persons who were nominated. These datasets have the purpose of gauging the popularity of films and the persons involved with the film.

\subsection{Transforming}
For the transformation process, the group took into account null and mismatched values and transformed them accordingly. For null values, it will be set to a default value or kept as null depending on the constraint of the data warehouse table. The group decided to use the \textit{INSERT IGNORE INTO} command for some of the extraction queries to disallow the script from adding mismatched values since only a few rows have mismatched values and all edge cases cannot be easily dealt with.

Multiple columns in the datasets contain string array values separated by commas that the group needed to account for. For string arrays that have a fixed amount of values such as genres and professions, the group decided to use one-hot encoding to query the values quicker without having to join with another table or generate more rows for each value. Rather than having multiple columns that corresponds to each value, which would take up too much space, the one-hot encoding is formed from a fixed length string, where each character in the string corresponds to a genre or profession with a \textbf{T} or \textbf{F} value, indicating true or false respectively. This way, the database can maintain the one-hot encoding while the backend of the application can interpret the value. To execute this in SQL, the group created a function that loops across the comma separated string, getting each value using the \textbf{STRING\textunderscore INDEX} function and concatenating a \textbf{T} or \textbf{F} value to the one-hot encoding string. The result of the string will then be added to a column on their respective tables.

For string arrays that do not have a fixed amount of values, such as the person keys from the crews and Oscars table, each person will be separated and have a separate row to allow for querying the foreign key values. This is done by using the \textit{RECURSIVE} statement to iterate over each value in the comma separated string, where each value is gotten from the \textbf{SUBSTRING\textunderscore INDEX} function. Each value gotten from the comma separated string will then be unioned then inserted to its respective table.

Since the fact tables were made with faster querying in mind, then some of its values have to be queried from the other tables in the data warehouse, leading to a potentially longer execution time.

The group added foreign key constraints to various keys to maintain data integrity, thus the group also used the \textit{INSERT IGNORE INTO} command to the queries to disallow values that don't follow the constraints from being added.

\subsection{Loading}
Initially, the group wanted to load the datasets into the data warehouse using Python, however its operations proved to be too slow and complicated to load the large amount of data in a reasonable amount of time. For example to load the original dataset into a source table the time it took to load the dataset was around seven hours. The group decided instead on using pure SQL to load the datasets to a source MySQL database which took two hours.

Additionally, a major problem the group encountered during the loading process was accounting for null and mismatched values. Across the various datasets, there we many empty values even on common values like names and titles. There were also values that are \textbf{\\N}, which also indicates that there is no value for that cell. There were mismatched and uncleaned values along the datasets such as having the value \textit{Reality-TV} be on the \textit{runtimeMinutes} column rather than the \textit{genres} column. To make up for all different values, the datatypes of all source tables are set to \textit{TEXT}, making the actual database after transfer take up two times more storage than the total storage across the datasets. 

The ETL process takes a long time to execute due to the sheer amount of data across all datasets that the script needs to go through. As a result, the group ran into issues when trying to execute the script. Our database is hosted in a server, so if the server goes down for any reason, the ETL script would stop. The group tried tackling these issues by making the overall process faster, like shifting from Python to SQL to faster querying and using a computer with a stronger CPU rather than a Raspberry Pi. Besides these optimizations, there was not much the group can do but wait for all the processes to finish.
